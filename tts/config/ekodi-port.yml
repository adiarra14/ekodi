# ── Ekodi Bambara TTS – Configuration ─────────────────────────
# This file controls dataset prep, training, inference, and deployment.

project:
  name: "ekodi-bambara-tts"
  language: "bm"            # ISO 639-1 code for Bambara / Bamanankan
  speaker_name: "ekodi"     # name for the single-speaker MVP voice

# ── Model ─────────────────────────────────────────────────────
model:
  # Meta MMS-TTS for Bambara (VITS architecture, end-to-end, no separate vocoder)
  # Pretrained on Bambara data by Facebook Research / Meta
  base_model: "facebook/mms-tts-bam"
  architecture: "vits"        # VITS = end-to-end (no separate vocoder needed)

# ── Data ──────────────────────────────────────────────────────
data:
  # Where raw downloads land
  raw_dir: "./data/raw"
  # Where processed (normalised) data goes
  processed_dir: "./data/processed"
  # Metadata CSV expected columns: file_path | text | speaker_id | duration
  metadata_file: "./data/processed/metadata.csv"
  # Audio parameters
  sample_rate: 16000
  max_duration_sec: 10.0
  min_duration_sec: 0.5
  # Train / validation split ratio
  val_ratio: 0.05

  # ── Dataset sources (tried in order) ──────────────────────
  sources:
    - name: "MALIBA-AI/bambara-asr-data"
      type: "huggingface"
      # Bambara ASR dataset – has aligned audio + text, usable for TTS
    - name: "oza75/bambara-tts"
      type: "huggingface"
      # Community Bambara TTS dataset
    - name: "mozilla-foundation/common_voice_16_0"
      type: "huggingface"
      subset: "bm"
      # Common Voice Bambara subset (if it exists)

# ── Training ──────────────────────────────────────────────────
training:
  output_dir: "./checkpoints"
  num_epochs: 50
  batch_size: 4
  gradient_accumulation_steps: 8      # effective batch = 32
  learning_rate: 1.0e-5
  warmup_steps: 500
  weight_decay: 0.01
  fp16: true                          # mixed precision – fits 8-12 GB VRAM
  save_steps: 500
  eval_steps: 500
  logging_steps: 50
  save_total_limit: 3
  dataloader_num_workers: 2
  seed: 42

# ── Inference ─────────────────────────────────────────────────
inference:
  # Path to fine-tuned checkpoint (set after training)
  checkpoint: null                    # e.g. "./checkpoints/best"
  # Fall back to base model if checkpoint is null
  max_length: 600                     # max token length for generation
  device: "auto"                      # "cpu", "cuda", or "auto"

# ── Server ────────────────────────────────────────────────────
server:
  host: "0.0.0.0"
  port: 8000
  cors_origins: ["*"]
  cache_size: 128                     # LRU cache entries for repeated prompts

# ── Hugging Face Hub ──────────────────────────────────────────
hub:
  repo_id: null                       # e.g. "your-username/ekodi-bambara-tts"
  private: false

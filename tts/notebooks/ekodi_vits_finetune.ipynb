{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ekodi â€” Fine-tune MMS-TTS (VITS) for Bambara Voice\n",
        "\n",
        "This notebook trains a custom Bambara TTS model on Google Colab.\n",
        "\n",
        "**What it does:**\n",
        "1. Clones the Ekodi repo from GitHub (code + voice samples)\n",
        "2. Downloads 10K+ Bambara speech samples from HuggingFace\n",
        "3. Includes your custom voice recordings from the repo\n",
        "4. Fine-tunes Facebook MMS-TTS-BAM (VITS architecture)\n",
        "5. Pushes the trained model to HuggingFace Hub\n",
        "6. Commits training results back to GitHub\n",
        "\n",
        "**Requirements:** Google Colab with GPU (free T4 works)\n",
        "\n",
        "**Repo:** https://github.com/adiarra14/ekodi\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 0. Setup â€” Check GPU & Install Dependencies\n",
        "!nvidia-smi\n",
        "import torch\n",
        "print(f\"\\nPyTorch {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected! Go to Runtime > Change runtime type > GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 1. Clone repo & install packages\n",
        "import os\n",
        "\n",
        "# Clone the Ekodi repo from GitHub\n",
        "REPO_URL = \"https://github.com/adiarra14/ekodi.git\"\n",
        "REPO_DIR = \"/content/ekodi\"\n",
        "\n",
        "if not os.path.exists(REPO_DIR):\n",
        "    !git clone {REPO_URL} {REPO_DIR}\n",
        "    print(f\"Cloned {REPO_URL}\")\n",
        "else:\n",
        "    !cd {REPO_DIR} && git pull\n",
        "    print(f\"Updated {REPO_DIR}\")\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(f\"{REPO_DIR}/tts\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers datasets soundfile librosa numpy scipy \\\n",
        "    huggingface_hub accelerate torchaudio pydub pyyaml\n",
        "\n",
        "# Convert custom voice samples (.m4a -> .wav) if not already done\n",
        "!apt-get install -qq ffmpeg > /dev/null 2>&1\n",
        "print(\"Packages + ffmpeg installed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2. Login to HuggingFace\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Option A: Use Colab Secrets (recommended â€” no token in code!)\n",
        "# Go to: Colab left panel > ðŸ”‘ Secrets > Add \"HF_TOKEN\" with your token\n",
        "try:\n",
        "    HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "    print(\"Token loaded from Colab Secrets\")\n",
        "except Exception:\n",
        "    # Option B: Paste your token here (will prompt if empty)\n",
        "    HF_TOKEN = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "else:\n",
        "    login()  # Interactive prompt\n",
        "\n",
        "print(\"HuggingFace login OK!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3. Configuration\n",
        "import os\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Load config from the repo\n",
        "cfg = yaml.safe_load(Path(\"config/ekodi-port.yml\").read_text())\n",
        "\n",
        "# === Training settings ===\n",
        "BASE_MODEL = cfg[\"model\"][\"base_model\"]     # \"facebook/mms-tts-bam\"\n",
        "HUB_REPO = \"adiarra14/ekodi-bambara-tts\"    # Where to push trained model\n",
        "MAX_PUBLIC_SAMPLES = 10000                    # How many public dataset samples\n",
        "USE_CUSTOM_VOICE = True                       # Include your voice recordings\n",
        "EPOCHS = 5                                    # Training epochs\n",
        "BATCH_SIZE = 4                                # Batch size (4 works on T4 16GB)\n",
        "LEARNING_RATE = 1e-4                          # Learning rate\n",
        "MAX_AUDIO_SEC = 10.0                          # Max audio duration in seconds\n",
        "TARGET_SR = cfg[\"data\"].get(\"sample_rate\", 16000)\n",
        "\n",
        "# Directories\n",
        "RAW_DIR = Path(\"data/raw\")\n",
        "CUSTOM_DIR = Path(\"data/custom_voice\")\n",
        "PROC_DIR = Path(\"data/processed\")\n",
        "CKPT_DIR = Path(\"checkpoints\")\n",
        "for d in [RAW_DIR, CUSTOM_DIR, PROC_DIR, CKPT_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Check if custom voice samples exist in the repo\n",
        "voice_dir = Path(\"assets/voice\")\n",
        "m4a_files = list(voice_dir.glob(\"*.m4a\")) if voice_dir.exists() else []\n",
        "print(f\"Base model:    {BASE_MODEL}\")\n",
        "print(f\"Hub repo:      {HUB_REPO}\")\n",
        "print(f\"Public data:   up to {MAX_PUBLIC_SAMPLES} samples\")\n",
        "print(f\"Custom voice:  {len(m4a_files)} .m4a files in assets/voice/\")\n",
        "print(f\"Config loaded from: config/ekodi-port.yml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4a. Download public Bambara speech data\n",
        "from datasets import load_dataset, Audio\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "print(\"Downloading OumarDicko/Bambara_AudioSynthetique_42K_V3...\")\n",
        "ds = load_dataset(\"OumarDicko/Bambara_AudioSynthetique_42K_V3\", split=\"train\", streaming=True)\n",
        "\n",
        "public_records = []\n",
        "skipped = 0\n",
        "\n",
        "for i, example in enumerate(ds):\n",
        "    if i >= MAX_PUBLIC_SAMPLES:\n",
        "        break\n",
        "\n",
        "    text = example.get(\"sentence\") or example.get(\"text\") or example.get(\"transcription\", \"\")\n",
        "    if not text or not text.strip():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    audio = example.get(\"audio\")\n",
        "    if audio is None:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    wav_path = RAW_DIR / f\"{i:06d}.wav\"\n",
        "\n",
        "    try:\n",
        "        if isinstance(audio, dict) and \"array\" in audio:\n",
        "            arr = np.array(audio[\"array\"], dtype=np.float32)\n",
        "            sr = audio.get(\"sampling_rate\", 16000)\n",
        "            sf.write(str(wav_path), arr, sr)\n",
        "        elif isinstance(audio, dict) and \"bytes\" in audio and audio[\"bytes\"]:\n",
        "            arr, sr = sf.read(io.BytesIO(audio[\"bytes\"]), dtype=\"float32\")\n",
        "            if arr.ndim > 1:\n",
        "                arr = arr.mean(axis=1)\n",
        "            sf.write(str(wav_path), arr, sr)\n",
        "        else:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        public_records.append({\"file_path\": str(wav_path), \"text\": text.strip(), \"source\": \"public\"})\n",
        "\n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print(f\"  ... {i+1} processed, {len(public_records)} saved\")\n",
        "    except Exception:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "print(f\"\\nPublic data: {len(public_records)} samples ({skipped} skipped)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4b. Convert custom voice samples from repo\n",
        "#\n",
        "# Voice samples are in the repo at assets/voice/*.m4a\n",
        "# Transcriptions are at assets/voice/transcriptions.csv\n",
        "# This cell converts m4a -> wav and loads the transcriptions.\n",
        "\n",
        "import csv\n",
        "import subprocess\n",
        "\n",
        "custom_records = []\n",
        "\n",
        "if USE_CUSTOM_VOICE and m4a_files:\n",
        "    print(f\"Converting {len(m4a_files)} voice samples from assets/voice/ ...\")\n",
        "    CUSTOM_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Convert m4a -> wav\n",
        "    converted = 0\n",
        "    wav_map = {}  # m4a_filename -> wav_path\n",
        "    for m4a in sorted(m4a_files):\n",
        "        wav_path = CUSTOM_DIR / (m4a.stem + \".wav\")\n",
        "        try:\n",
        "            subprocess.run([\n",
        "                \"ffmpeg\", \"-y\", \"-i\", str(m4a),\n",
        "                \"-ar\", str(TARGET_SR), \"-ac\", \"1\", \"-sample_fmt\", \"s16\",\n",
        "                \"-f\", \"wav\", str(wav_path)\n",
        "            ], capture_output=True, timeout=30, check=True)\n",
        "            wav_map[m4a.name] = str(wav_path)\n",
        "            converted += 1\n",
        "        except Exception as e:\n",
        "            print(f\"  Failed: {m4a.name} ({e})\")\n",
        "    print(f\"  Converted {converted}/{len(m4a_files)} files to WAV\")\n",
        "\n",
        "    # Load transcriptions from assets/voice/ (committed to git)\n",
        "    trans_path = Path(\"assets/voice/transcriptions.csv\")\n",
        "    if trans_path.exists():\n",
        "        with open(trans_path, encoding=\"utf-8\") as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                text = row.get(\"text\", \"\").strip()\n",
        "                fname = row.get(\"file_name\", \"\").strip()\n",
        "                if not text:\n",
        "                    continue\n",
        "                # Match m4a filename to converted wav\n",
        "                if fname in wav_map:\n",
        "                    custom_records.append({\n",
        "                        \"file_path\": wav_map[fname],\n",
        "                        \"text\": text,\n",
        "                        \"source\": \"custom\",\n",
        "                    })\n",
        "        print(f\"  Loaded {len(custom_records)} transcribed samples from assets/voice/transcriptions.csv\")\n",
        "    else:\n",
        "        print(\"  No transcriptions.csv found at assets/voice/transcriptions.csv\")\n",
        "        print(\"  TIP: Add transcriptions and push to git\")\n",
        "else:\n",
        "    print(\"No custom voice files found in assets/voice/\")\n",
        "\n",
        "print(f\"\\nTotal: {len(public_records)} public + {len(custom_records)} custom = {len(public_records) + len(custom_records)} samples\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5. Preprocess all data\n",
        "import unicodedata\n",
        "import re\n",
        "import librosa\n",
        "import random\n",
        "\n",
        "def normalize_bambara(text):\n",
        "    \"\"\"Bambara text normalization.\"\"\"\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    text = text.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\")\n",
        "    text = text.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "all_records = public_records + custom_records\n",
        "processed = []\n",
        "\n",
        "for rec in all_records:\n",
        "    try:\n",
        "        audio, sr = librosa.load(rec[\"file_path\"], sr=TARGET_SR)\n",
        "        dur = len(audio) / TARGET_SR\n",
        "        if dur < 0.5 or dur > MAX_AUDIO_SEC:\n",
        "            continue\n",
        "\n",
        "        text = normalize_bambara(rec[\"text\"])\n",
        "        if len(text) < 2:\n",
        "            continue\n",
        "\n",
        "        out_path = PROC_DIR / Path(rec[\"file_path\"]).name\n",
        "        sf.write(str(out_path), audio, TARGET_SR)\n",
        "\n",
        "        processed.append({\n",
        "            \"file_path\": str(out_path),\n",
        "            \"text\": text,\n",
        "            \"source\": rec.get(\"source\", \"public\"),\n",
        "        })\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(processed)\n",
        "split_idx = int(len(processed) * 0.95)\n",
        "train_records = processed[:split_idx]\n",
        "val_records = processed[split_idx:]\n",
        "\n",
        "n_custom = sum(1 for r in train_records if r[\"source\"] == \"custom\")\n",
        "print(f\"Processed: {len(processed)} total\")\n",
        "print(f\"  Train: {len(train_records)} ({n_custom} custom)\")\n",
        "print(f\"  Val:   {len(val_records)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 6. Load model\n",
        "from transformers import VitsModel, AutoTokenizer\n",
        "\n",
        "print(f\"Loading {BASE_MODEL}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "model = VitsModel.from_pretrained(BASE_MODEL).to(\"cuda\")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model: {total_params/1e6:.1f}M params, SR={model.config.sampling_rate}Hz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 7. Test BEFORE fine-tuning\n",
        "import IPython.display as ipd\n",
        "\n",
        "test_texts = [\n",
        "    \"I ni ce\",\n",
        "    \"Aw ni ce, ne togo ye Ekodi\",\n",
        "    \"Bamanankan ye kan nafama ye\",\n",
        "    \"Lakoli ye yoro \\u0272uman ye denmis\\u025bnw ye\",\n",
        "]\n",
        "\n",
        "print(\"=== BEFORE fine-tuning ===\")\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "    wav = output.waveform[0].cpu().numpy()\n",
        "    print(f'\\n\"{text}\"  ({len(wav)/model.config.sampling_rate:.1f}s)')\n",
        "    ipd.display(ipd.Audio(wav, rate=model.config.sampling_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 8. Setup training\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "\n",
        "# Mel spectrogram transform\n",
        "mel_fn = T.MelSpectrogram(\n",
        "    sample_rate=model.config.sampling_rate,\n",
        "    n_fft=1024, hop_length=256, n_mels=80\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "class BambaraDataset(Dataset):\n",
        "    def __init__(self, records, tokenizer, sr=16000, max_len=None):\n",
        "        self.records = records\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sr = sr\n",
        "        self.max_len = max_len or int(MAX_AUDIO_SEC * sr)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rec = self.records[idx]\n",
        "        audio, _ = sf.read(rec[\"file_path\"], dtype=\"float32\")\n",
        "        if audio.ndim > 1:\n",
        "            audio = audio.mean(axis=1)\n",
        "        if len(audio) > self.max_len:\n",
        "            audio = audio[:self.max_len]\n",
        "        tokens = self.tokenizer(rec[\"text\"], return_tensors=\"pt\", padding=False)\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": tokens[\"attention_mask\"].squeeze(0),\n",
        "            \"waveform\": torch.tensor(audio, dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    max_t = max(b[\"input_ids\"].shape[0] for b in batch)\n",
        "    max_a = max(b[\"waveform\"].shape[0] for b in batch)\n",
        "    ids = torch.zeros(len(batch), max_t, dtype=torch.long)\n",
        "    mask = torch.zeros(len(batch), max_t, dtype=torch.long)\n",
        "    wavs = torch.zeros(len(batch), max_a)\n",
        "    for i, b in enumerate(batch):\n",
        "        tl = b[\"input_ids\"].shape[0]\n",
        "        al = b[\"waveform\"].shape[0]\n",
        "        ids[i, :tl] = b[\"input_ids\"]\n",
        "        mask[i, :tl] = b[\"attention_mask\"]\n",
        "        wavs[i, :al] = b[\"waveform\"]\n",
        "    return {\"input_ids\": ids, \"attention_mask\": mask, \"waveforms\": wavs}\n",
        "\n",
        "\n",
        "train_ds = BambaraDataset(train_records, tokenizer, model.config.sampling_rate)\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    collate_fn=collate, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# Freeze everything except embeddings + duration predictor + projection\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if any(k in name for k in [\"embed_tokens\", \"duration_predictor\", \"proj\"]):\n",
        "        p.requires_grad = True\n",
        "\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable: {trainable:,} / {total_params:,} ({100*trainable/total_params:.1f}%)\")\n",
        "print(f\"Dataset: {len(train_ds)} train samples\")\n",
        "print(f\"Batches/epoch: {len(train_loader)}\")\n",
        "print(f\"Epochs: {EPOCHS}, LR: {LEARNING_RATE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 9. TRAIN! ðŸš€\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LEARNING_RATE, weight_decay=0.01\n",
        ")\n",
        "\n",
        "ACCUM_STEPS = 4  # Effective batch = BATCH_SIZE * ACCUM_STEPS\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "print(f\"Starting training: {EPOCHS} epochs, batch={BATCH_SIZE}, accum={ACCUM_STEPS}\")\n",
        "print(f\"Effective batch size: {BATCH_SIZE * ACCUM_STEPS}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    n_batches = 0\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "        mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "        target = batch[\"waveforms\"].to(\"cuda\")\n",
        "\n",
        "        try:\n",
        "            # Forward: generate audio\n",
        "            output = model(input_ids=ids, attention_mask=mask)\n",
        "            pred = output.waveform\n",
        "\n",
        "            # Mel-spectrogram loss\n",
        "            min_len = min(pred.shape[-1], target.shape[-1])\n",
        "            if min_len < 1024:\n",
        "                continue\n",
        "\n",
        "            pred_mel = torch.log(mel_fn(pred[..., :min_len]).clamp(min=1e-5))\n",
        "            tgt_mel = torch.log(mel_fn(target[..., :min_len]).clamp(min=1e-5))\n",
        "\n",
        "            l1 = F.l1_loss(pred_mel, tgt_mel)\n",
        "            sc = torch.norm(tgt_mel - pred_mel) / (torch.norm(tgt_mel) + 1e-7)\n",
        "            loss = (l1 + sc) / ACCUM_STEPS\n",
        "\n",
        "            loss.backward()\n",
        "            epoch_loss += loss.item() * ACCUM_STEPS\n",
        "            n_batches += 1\n",
        "\n",
        "            if (batch_idx + 1) % ACCUM_STEPS == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad], 1.0\n",
        "                )\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"  [{batch_idx+1}/{len(train_loader)}] loss={loss.item()*ACCUM_STEPS:.4f}\")\n",
        "\n",
        "        except torch.cuda.OutOfMemoryError:\n",
        "            torch.cuda.empty_cache()\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            if batch_idx < 3:\n",
        "                print(f\"  Error batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg = epoch_loss / max(n_batches, 1)\n",
        "    dt = time.time() - t0\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | loss={avg:.4f} | batches={n_batches} | time={dt:.0f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if avg < best_loss and n_batches > 0:\n",
        "        best_loss = avg\n",
        "        model.save_pretrained(str(CKPT_DIR / \"best\"))\n",
        "        tokenizer.save_pretrained(str(CKPT_DIR / \"best\"))\n",
        "        print(f\"  >>> New best model saved! (loss={best_loss:.4f})\")\n",
        "\n",
        "# Final save\n",
        "model.save_pretrained(str(CKPT_DIR / \"final\"))\n",
        "tokenizer.save_pretrained(str(CKPT_DIR / \"final\"))\n",
        "print(f\"\\nTraining complete! Best loss: {best_loss:.4f}\")\n",
        "print(f\"Checkpoints: {CKPT_DIR}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 10. Test AFTER fine-tuning\n",
        "print(\"=== AFTER fine-tuning ===\")\n",
        "model.eval()\n",
        "\n",
        "for text in test_texts:\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output = model(**tokens)\n",
        "    wav = output.waveform[0].cpu().numpy()\n",
        "    print(f'\\n\"{text}\"  ({len(wav)/model.config.sampling_rate:.1f}s)')\n",
        "    ipd.display(ipd.Audio(wav, rate=model.config.sampling_rate))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 11. Push to HuggingFace Hub ðŸš€\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# Load best checkpoint\n",
        "best_path = CKPT_DIR / \"best\"\n",
        "if not best_path.exists():\n",
        "    best_path = CKPT_DIR / \"final\"\n",
        "\n",
        "print(f\"Pushing {best_path} to {HUB_REPO}...\")\n",
        "\n",
        "best_model = VitsModel.from_pretrained(str(best_path))\n",
        "best_tokenizer = AutoTokenizer.from_pretrained(str(best_path))\n",
        "\n",
        "best_model.push_to_hub(HUB_REPO, private=False)\n",
        "best_tokenizer.push_to_hub(HUB_REPO)\n",
        "\n",
        "# Create model card\n",
        "api = HfApi()\n",
        "card = f\"\"\"---\n",
        "language: bm\n",
        "license: cc-by-nc-4.0\n",
        "tags:\n",
        "  - tts\n",
        "  - bambara\n",
        "  - vits\n",
        "  - mms\n",
        "  - ekodi\n",
        "pipeline_tag: text-to-speech\n",
        "---\n",
        "\n",
        "# Ekodi Bambara TTS\n",
        "\n",
        "Fine-tuned MMS-TTS model for Bambara (Bamanankan) text-to-speech.\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from transformers import VitsModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model = VitsModel.from_pretrained(\"{HUB_REPO}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{HUB_REPO}\")\n",
        "\n",
        "text = \"I ni ce\"  # Hello in Bambara\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    output = model(**inputs)\n",
        "waveform = output.waveform[0].numpy()\n",
        "# waveform is 16kHz float32 audio\n",
        "```\n",
        "\n",
        "## Base model\n",
        "- facebook/mms-tts-bam (Meta MMS-TTS, VITS architecture)\n",
        "\n",
        "## Training data\n",
        "- OumarDicko/Bambara_AudioSynthetique_42K_V3\n",
        "- Custom Bambara voice recordings\n",
        "\n",
        "## Project\n",
        "- GitHub: https://github.com/adiarra14/ekodi\n",
        "\"\"\"\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=card.encode(),\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=HUB_REPO,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Model pushed to: https://huggingface.co/{HUB_REPO}\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nUse it with:\")\n",
        "print(f'  model = VitsModel.from_pretrained(\"{HUB_REPO}\")')\n",
        "print(f'  tokenizer = AutoTokenizer.from_pretrained(\"{HUB_REPO}\")')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 12. Push training results back to GitHub\n",
        "import subprocess\n",
        "\n",
        "# Configure git\n",
        "GITHUB_USER = \"adiarra14\"\n",
        "GITHUB_REPO = \"ekodi\"\n",
        "# For private repos, use a personal access token:\n",
        "# GITHUB_TOKEN = \"ghp_...\"  # Uncomment and paste your GitHub token\n",
        "# !git remote set-url origin https://{GITHUB_TOKEN}@github.com/{GITHUB_USER}/{GITHUB_REPO}.git\n",
        "\n",
        "os.chdir(f\"/content/ekodi\")\n",
        "\n",
        "# Git config\n",
        "!git config user.email \"adiarra@gmail.com\"\n",
        "!git config user.name \"adiarra14\"\n",
        "\n",
        "# Add training results (not the large data/checkpoint files)\n",
        "!echo \"tts/data/\" >> .gitignore\n",
        "!echo \"tts/checkpoints/\" >> .gitignore\n",
        "\n",
        "# Copy best model to a lightweight location\n",
        "best_info = f\"Training complete. Best loss: {best_loss:.4f}\"\n",
        "with open(\"tts/TRAINING_LOG.md\", \"w\") as f:\n",
        "    f.write(f\"# Ekodi Training Log\\n\\n\")\n",
        "    f.write(f\"- **Date**: {time.strftime('%Y-%m-%d %H:%M')}\\n\")\n",
        "    f.write(f\"- **Base model**: {BASE_MODEL}\\n\")\n",
        "    f.write(f\"- **Epochs**: {EPOCHS}\\n\")\n",
        "    f.write(f\"- **Best loss**: {best_loss:.4f}\\n\")\n",
        "    f.write(f\"- **Train samples**: {len(train_records)} ({sum(1 for r in train_records if r.get('source')=='custom')} custom)\\n\")\n",
        "    f.write(f\"- **HuggingFace model**: https://huggingface.co/{HUB_REPO}\\n\")\n",
        "    f.write(f\"\\n## Usage\\n\\n```python\\nfrom transformers import VitsModel, AutoTokenizer\\n\")\n",
        "    f.write(f'model = VitsModel.from_pretrained(\"{HUB_REPO}\")\\n')\n",
        "    f.write(f'tokenizer = AutoTokenizer.from_pretrained(\"{HUB_REPO}\")\\n```\\n')\n",
        "\n",
        "!git add -A\n",
        "!git status\n",
        "!git commit -m \"Add training results â€” best loss {best_loss:.4f}\"\n",
        "\n",
        "print(\"\\nTo push to GitHub, run:\")\n",
        "print(f\"  !git push origin main\")\n",
        "print(\"\\n(You may need to authenticate â€” use a GitHub personal access token)\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}